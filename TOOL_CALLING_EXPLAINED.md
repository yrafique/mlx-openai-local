# Tool Calling: How It Works

## â“ Your Question
> "Why isn't the tool getting executed?"

## âœ… Answer: It IS Working Correctly!

The MLX Omni Server implements the **OpenAI-compatible API standard**, which means:

1. **The server ONLY generates tool calls** (with 99% accuracy)
2. **The client MUST execute the tools** (your application code)
3. **The client sends results back** to get the final answer

This is the standard OpenAI API behavior - it's not a bug, it's by design!

## ğŸ“Š The Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: User â†’ LLM                                         â”‚
â”‚  "What is 123 * 456?"                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: LLM â†’ Client                                       â”‚
â”‚  {                                                           â”‚
â”‚    "tool_calls": [{                                         â”‚
â”‚      "function": {                                          â”‚
â”‚        "name": "calculate",                                 â”‚
â”‚        "arguments": "{\"expr\": \"123 * 456\"}"             â”‚
â”‚      }                                                       â”‚
â”‚    }]                                                        â”‚
â”‚  }                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Client Executes Tool                               â”‚
â”‚  result = eval("123 * 456")  # = 56088                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Client â†’ LLM (with result)                         â”‚
â”‚  {                                                           â”‚
â”‚    "role": "tool",                                          â”‚
â”‚    "content": "56088"                                       â”‚
â”‚  }                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: LLM â†’ Client (final answer)                        â”‚
â”‚  "The answer is 56088."                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” What You Observed

Your curl command:
```bash
curl -X POST http://localhost:7007/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/Llama-3.2-3B-Instruct-4bit",
    "messages": [{"role": "user", "content": "What is 123 * 456?"}],
    "tools": [{"type": "function", "function": {"name": "calculate", ...}}],
    "tool_choice": "auto"
  }'
```

**Response** (this is CORRECT):
```json
{
  "tool_calls": [{
    "function": {
      "name": "calculate",
      "arguments": "{\"expr\": \"123 * 456\"}"
    }
  }],
  "finish_reason": "tool_calls"
}
```

This means: **"I need to call the calculate function with 123 * 456"**

The server did its job! Now YOU must:
1. Execute `calculate("123 * 456")` â†’ get `56088`
2. Send that result back
3. Get the final answer

## âœ… Working Example

Run this:
```bash
python3 test_tool_execution.py
```

Output:
```
ğŸ“¤ Step 1: Sending request to LLM with tools...
ğŸ“¥ Step 1 Response: finish_reason = tool_calls

ğŸ”§ Step 2: LLM requested tool execution!
   - Function: calculate
   - Arguments: {'expr': '123 * 456'}
   - Result: 56088

ğŸ“¤ Step 3: Sending tool results back to LLM...
ğŸ“¥ Step 3 Response: finish_reason = stop

âœ… FINAL ANSWER: The answer is 56088.
```

## ğŸ¯ Why This Design?

**Security & Flexibility:**
- Server doesn't execute arbitrary code (security risk)
- Client controls what tools are available
- Client can add authentication, rate limiting, etc.
- Works across different programming languages

**Standard Pattern:**
- OpenAI API works this way
- LangChain works this way
- All major LLM APIs work this way

## ğŸš€ How to Use in Your App

### Option 1: Use OpenAI SDK (Recommended)
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:7007/v1",
    api_key="local-demo-key"
)

# See test_tool_execution.py for complete example
```

### Option 2: Use LangChain
```python
from langchain_openai import ChatOpenAI
from langchain.agents import tool, create_tool_calling_agent

# See examples/langchain_function_calling.py
```

### Option 3: Your UI Does This Automatically
The Streamlit UI (`http://localhost:7006`) handles this flow automatically:
- You ask a question
- UI sends request to LLM
- UI executes tools
- UI sends results back
- You see the final answer

That's why it "just works" in the UI!

## ğŸ“š References

- MLX Omni Server: https://github.com/madroidmaq/mlx-omni-server
- OpenAI Function Calling: https://platform.openai.com/docs/guides/function-calling
- Your working example: `test_tool_execution.py`

## ğŸ“ Key Takeaway

**Your server is working perfectly!** The 99% function calling accuracy means the model correctly identifies when and how to call tools. The execution is YOUR responsibility (by design).
